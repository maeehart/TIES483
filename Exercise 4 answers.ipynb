{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_constrained(x):\n",
    "    return x[0]**2+x[1]**2+x[0]+2*x[1], [], [x[0]+x[1]-1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alpha(x,f):\n",
    "    (_,ieq,eq) = f(x)\n",
    "    return sum([min([0,ieq_j])**2 for ieq_j in ieq])\\\n",
    "+sum([eq_k**2 for eq_k in eq])\n",
    "\n",
    "def penalized_function(x,f,r):\n",
    "    return f(x)[0] + r*alpha(x,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us solve the penalized problem with penalty term growin in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.74340989  0.24336301] 95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "r = 0\n",
    "x_old = np.array([float('inf')]*2)\n",
    "x_new = [0,0]\n",
    "while np.linalg.norm(x_new-x_old)>0.0001:\n",
    "    res = minimize(lambda x:penalized_function(x,f_constrained,r),\n",
    "               [0,0],method='Nelder-Mead')\n",
    "    x_old = x_new\n",
    "    x_new = np.array(res.x)\n",
    "    r = r+1\n",
    "print x_new, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_constrained_approx(x,epsilon):\n",
    "    return x[0]**2+x[1]**2+x[0]+2*x[1], [x[0]+x[1]-1+epsilon,\\\n",
    "                                         epsilon-(x[0]+x[1]-1)], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beta(x,f):\n",
    "    _,ieq,_ = f(x)\n",
    "    try:\n",
    "        value=sum([1/max([0,ieq_j]) for ieq_j in ieq])\n",
    "    except ZeroDivisionError:\n",
    "        value = float(\"inf\")\n",
    "    return value\n",
    "def function_with_barrier(x,f,r):\n",
    "    return f(x)[0]+r*beta(x,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5578683  0.0578683] 0.5 0.5\n",
      "[ 0.68545665  0.18545665] 0.25 0.25\n",
      "[ 0.73158293  0.23158293] 0.125 0.125\n",
      "[ 0.74519333  0.24519333] 0.0625 0.0625\n",
      "[ 0.74878417  0.24878417] 0.03125 0.03125\n",
      "[ 0.74969513  0.24969513] 0.015625 0.015625\n",
      "[ 0.74992373  0.24992373] 0.0078125 0.0078125\n",
      "[ 0.74998093  0.24998093] 0.00390625 0.00390625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ad\n",
    "from scipy.optimize import minimize\n",
    "r = 1.0\n",
    "epsilon = 1.\n",
    "x_old = np.array([float('inf')]*2)\n",
    "x_new = [1,0]\n",
    "while np.linalg.norm(x_new-x_old)>0.0001:\n",
    "    g = lambda x: function_with_barrier(x,\\\n",
    "               lambda y: f_constrained_approx(y,epsilon),r)\n",
    "    res = minimize(g,x_new,method='Newton-CG',jac=ad.gh(g)[0],\\\n",
    "                   hess=ad.gh(g)[1])\n",
    "    x_old = x_new\n",
    "    x_new = res.x\n",
    "    r=r/2\n",
    "    epsilon = epsilon/2\n",
    "    print x_new, epsilon, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def project_vector(A,vector):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ad\n",
    "def projected_gradient_method(f,A,start,step,precision):\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        f_old = f_new\n",
    "        gradient = ad.gh(f)[0](x)\n",
    "        grad_proj = project_vector(A,[-i for i in gradient])#The only changes to steepest..\n",
    "        grad_proj = np.array(grad_proj.transpose())[0] #... descent are here!\n",
    "        x = x+grad_proj*step\n",
    "        f_new = f(x)\n",
    "        steps.append(list(x))\n",
    "    return x,f_new,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.75,  0.25]), 1.875, [[0.75, 0.25], [0.75, 0.25]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projected_gradient_method(lambda x:f_constrained(x)[0],[[1,1]],[1,0]\\\n",
    "                          ,.5,0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to show that there exists unique Lagrance multiplier vectors $\\lambda^* = (\\lambda^*_1,\\ldots,\\lambda_J^*)$ and $\\mu^*=(\\mu_1^*,\\ldots,\\mu_K^*)$ such that\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\nabla_xL(x,\\lambda,\\mu) = 0\\\\\n",
    "&\\mu_j^*\\geq0,\\text{ for all }j=1,\\ldots,J\\\\\n",
    "&\\mu_j^*g_j(x)=0,\\text{for all }j=1,\\ldots,J,\n",
    "\\end{align}\n",
    "$$\n",
    "where $$L(x,\\lambda,\\mu) = f(x)- \\sum_{k=1}^K\\mu_kg_k(x) -\\sum_{j=1}^J\\lambda_jh_j(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,  $f(x) = x_1^2+x_2^2+x_1+2x_2$, $g(x) = 0$ and $h(x)=x_1+x_2-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, stability rule becomes $$\n",
    "\\left\\{\n",
    "\\begin{align}\n",
    "2x_1+1-\\lambda = 0\\\\\n",
    "2x_2+2-\\lambda=0.\n",
    "\\end{align}\n",
    "\\right.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have a complementary rule, since we do not have inequality constraints!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_KKT_eqc(x,tol):\n",
    "    l = 2*x[0]+1\n",
    "    if abs(2*x[1]+2-l)<=tol:\n",
    "        print abs(2*x[1]+2-l)\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_KKT_eqc([0.74998093,0.24998093],0.000001)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
