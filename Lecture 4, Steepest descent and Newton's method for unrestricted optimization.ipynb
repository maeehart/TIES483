{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steepest descent and Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us define the same function as on the previous lesson for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f_simple(x):\n",
    "    return (x[0] - 10.0)**2 + (x[1] + 5.0)**2+x[0]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import automatic differentiation package for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ask for gradient and hessian using the <pre>ad.gh</pre> function. Let us do that for the function <it>f</it>  that we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grad_f, hess_f = ad.gh(f_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At the point (1,2) gradient is  [-16.0, 14.0]  and hessian is  [[4.0, 0.0], [0.0, 2.0]]\n"
     ]
    }
   ],
   "source": [
    "print \"At the point (1,2) gradient is \", grad_f([1,2]), \" and hessian is \",hess_f([1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base algorithm for the steepest descent and Newton's algorithms\n",
    "**Input:** function $f$ to be optimized, starting point $x_0$, step length rule $alpha$, stopping rule $stop$  \n",
    "**Output:** A solution $x^*$ that is close to a locally optimal solution\n",
    "```\n",
    "set f_old as a big number and f_new as f(x*)\n",
    "while a stopping criterion has not been met:\n",
    "    f_old = f_new\n",
    "    determine search direction d_h according to the method\n",
    "    determine the step length alpha\n",
    "    set x = x + alpha *d_h\n",
    "    f_new = f(x)\n",
    "return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to determine search direction distinguishes steepest descent algorithm and the Newton algorithm. Different stopping rules and step sizes can be mixed and matched with both algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest Descent algorithm for unconstrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the steepest descent algorithm, the search direction is determined by the negative of the gradient $-\\nabla f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use a simple stopping rule, where we stop when the change is not bigger than precision and step size is constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def steepest_descent(f,start,step,precision):\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        f_old = f_new\n",
    "        d = -np.array(ad.gh(f)[0](x))\n",
    "        x = x+d*step\n",
    "        f_new = f(x)\n",
    "        steps.append(list(x))\n",
    "    return x,f_new,steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Solve the problem using the Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution is  [ 5.         -5.00653035]\n"
     ]
    }
   ],
   "source": [
    "start = [2.0,-10.0]\n",
    "(x_value,f_value,steps) = steepest_descent(f_simple,start,0.2,0.0001)\n",
    "print \"Optimal solution is \",x_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the steps of solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_2d_steps(steps,start):\n",
    "    myvec = np.array([start]+steps).transpose()\n",
    "    plt.plot(myvec[0,],myvec[1,],'ro')\n",
    "    for label,x,y in zip([str(i) for i in range(len(steps)+1)],myvec[0,],myvec[1,]):\n",
    "        plt.annotate(label,xy = (x, y))\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Based on setting the research direction as $-[Hf(x)]^{-1}\\nabla f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one-dimensional case, it is easy to see that since\n",
    "$$f(x+\\Delta x)\\approx f(x)+f'(x)\\Delta x+\\frac12f''(x)\\Delta x^2$$\n",
    "with the Taylor series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want to find $x$ such that $f(x)$ is at minimum and, thus, we seek to solve the equation that sets the derivative of this expression with respect to $\\Delta x$ equal to zero:\n",
    "\n",
    "$$ 0 = \\frac{d}{d\\Delta x} \\left(f(x_n)+f'(x_n)\\Delta x+\\frac 1 2 f''(x_n) \\Delta x^2\\right) = f'(x_n)+f'' (x_n) \\Delta x.$$\n",
    "\n",
    "The solution of the above equation is $\\Delta x=-f'(x)/f''(x)$. Thus, the best approximation of $x$ as the minimum is $x-f'(x)/f''(x)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def newton(f,start,step,precision):\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        f_old = f_new\n",
    "        H_inv = np.linalg.inv(np.matrix(ad.gh(f)[1](x)))\n",
    "        d = (-H_inv*(np.matrix(ad.gh(f)[0](x)).transpose())).transpose()\n",
    "        x = np.array(x+d*step)[0] #Change the type back to array so that we can use it in our function\n",
    "        f_new = f(x)\n",
    "        steps.append(list(x))\n",
    "    return x,f_new,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution is  [ 5. -5.]\n"
     ]
    }
   ],
   "source": [
    "start = [2.0,-10.0]\n",
    "(x_value,f_value,steps) = newton(f_simple,start,1,0.01)\n",
    "print \"Optimal solution is \",x_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import test_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12101.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_problems.rosenbrock(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x_value,f_value,steps) = steepest_descent(test_problems.rosenbrock,start,0.1,1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.0, -10.0], [-438.20000000000005, 210.0], [3385118541.1600018, 3853942.8000000007], [-1.5516066618863538e+30, 2.2918055061862639e+20], [1.4941868092114043e+92, 4.8149664664202277e+61], [nan, nan]]\n"
     ]
    }
   ],
   "source": [
    "print [start]+steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
